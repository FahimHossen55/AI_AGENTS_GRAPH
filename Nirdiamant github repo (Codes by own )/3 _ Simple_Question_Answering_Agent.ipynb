{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Question-Answering Agent**\n",
        "## **Overview :**  \n",
        "This Code introduces a basic Question-Answering (QA) agent using LangChain and OpenAI's language model. The agent is designed to understand user queries and provide relevant, concise answers."
      ],
      "metadata": {
        "id": "pSAKfu8m3QKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Motivation**\n",
        "In the era of AI-driven interactions, creating a simple QA agent serves as a fundamental stepping stone towards more complex AI systems. This project aims to:\n",
        "\n",
        "- Demonstrate the basics of AI-driven question-answering\n",
        "- Introduce key concepts in building AI agents\n",
        "- Provide a foundation for more advanced agent architectures"
      ],
      "metadata": {
        "id": "qskDwN_r3pix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Components**\n",
        "1. Language Model: Utilizes OpenAI's GPT model for natural language understanding and generation.\n",
        "2. Prompt Template: Defines the structure and context for the agent's responses.\n",
        "3. LLMChain: Combines the language model and prompt template for streamlined processing."
      ],
      "metadata": {
        "id": "RfugYZuu3vcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method Details**\n",
        "## 1. Setup and Initialization\n",
        "- Import necessary libraries (LangChain, dotenv)\n",
        "- Load environment variables for API key management\n",
        "- Initialize the OpenAI language model\n",
        "## 2. Defining the Prompt Template\n",
        "- Create a template that instructs the AI on its role and response format\n",
        "- Use the PromptTemplate class to structure the input\n",
        "## 3. Creating the LLMChain\n",
        "- Combine the language model and prompt template into an LLMChain\n",
        "- This chain manages the flow from user input to AI response\n",
        "## 4. Implementing the Question-Answering Function\n",
        "- Define a function that takes a user question as input\n",
        "- Use the LLMChain to process the question and generate an answer\n",
        "## 5. User Interaction\n",
        "- In a Jupyter notebook environment, provide cells for:\n",
        "- Example usage with a predefined question\n",
        "- Interactive input for user questions"
      ],
      "metadata": {
        "id": "yV_J6cL236-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries\n"
      ],
      "metadata": {
        "id": "D_Lnbfso6B_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8Zsum0h3uzm",
        "outputId": "0f48fe91-ce57-45aa-8330-30addda4ba2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-core-0.3.9 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langchain_openai-0.2.1 langsmith-0.1.131 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.51.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6AMMDHgS2tDE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "load_dotenv()\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"open_ai_api\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialize the language model\n"
      ],
      "metadata": {
        "id": "ccyIpSnl5-Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1000, temperature=0)\n"
      ],
      "metadata": {
        "id": "tLW_vs195XKc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the prompt template\n"
      ],
      "metadata": {
        "id": "s1FX_rG-6El7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a helpful AI assistant. Your task is to answer the user's question to the best of your ability.\n",
        "\n",
        "User's question: {question}\n",
        "\n",
        "Please provide a clear and concise answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "E950yy-A58PR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the LLMChain\n"
      ],
      "metadata": {
        "id": "vJ79jRI-6N7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = prompt | llm\n"
      ],
      "metadata": {
        "id": "qCagy4fA6Lta"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the get_answer function\n"
      ],
      "metadata": {
        "id": "oUKX29Lv6SsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(question):\n",
        "    \"\"\"\n",
        "    Get an answer to the given question using the QA chain.\n",
        "    \"\"\"\n",
        "    input_variables = {\"question\": question}\n",
        "    response = qa_chain.invoke(input_variables).content\n",
        "    return response"
      ],
      "metadata": {
        "id": "EoN49rdd6Qhe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of France?\"\n",
        "answer = get_answer(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "7eNH4cUv6VCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question: What is the capital of France?\n",
        "\n",
        "Answer: The capital of France is Paris."
      ],
      "metadata": {
        "id": "h8TXjN966e4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTwT1AHv6Ylx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
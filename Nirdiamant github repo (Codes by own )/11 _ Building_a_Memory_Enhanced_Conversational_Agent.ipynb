{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Building a Memory-Enhanced Conversational Agent**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uJggtMlzVswZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This tutorial outlines the process of creating a conversational AI agent with enhanced memory capabilities. The agent incorporates both short-term and long-term memory to maintain context and improve the quality of interactions over time.\n",
        "\n",
        "## Motivation  \n",
        "Traditional chatbots often struggle with maintaining context beyond immediate interactions. This limitation can lead to disjointed conversations and a lack of personalization. By implementing both short-term and long-term memory, we aim to create an agent that can:\n",
        "\n",
        "- Maintain context within a single conversation\n",
        "- Remember important information across multiple sessions\n",
        "-  Provide more coherent and personalized responses\n",
        "\n",
        "## Key Components\n",
        "1.  Language Model: The core AI component for understanding and generating responses.\n",
        "2.  Short-term Memory: Stores the immediate conversation history.\n",
        "3.  Long-term Memory: Retains important information across multiple conversations.\n",
        "4.  Prompt Template: Structures the input for the language model, incorporating both types of memory.\n",
        "5.  Memory Manager: Handles the storage and retrieval of information in both memory types."
      ],
      "metadata": {
        "id": "5WdId4qvgydH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method Details\n",
        "## Setting Up the Environment\n",
        "   1. Import necessary libraries for the language model, memory management, and prompt handling.\n",
        "   2. Initialize the language model with desired parameters (e.g., model type, token limit).\n",
        "\n",
        "## Implementing Memory Systems\n",
        " 1. Create a store for short-term memory (conversation history):\n",
        "\n",
        "    -  Use a dictionary to manage multiple conversation sessions.\n",
        "    -  Implement a function to retrieve or create new conversation histories.\n",
        " 2. Develop a long-term memory system:\n",
        "\n",
        "    - Create a separate store for persistent information.\n",
        "    - Implement functions to update and retrieve long-term memories.\n",
        "    - Define simple criteria for what information to store long-term (e.g., longer user inputs).\n",
        "\n",
        "## Designing the Conversation Structure\n",
        "  1. Create a prompt template that includes:\n",
        "    - A system message defining the AI's role.\n",
        "    - A section for long-term memory context.\n",
        "    - A placeholder for the conversation history (short-term memory).\n",
        "The current user input.\n",
        "\n",
        "## Building the Conversational Chain\n",
        "\n",
        "   1. Combine the prompt template with the language model.\n",
        "   2. Wrap this combination with a component that manages message history.\n",
        "   3.  Ensure the chain can access and update both short-term and long-term memory."
      ],
      "metadata": {
        "id": "_6DIE6KEh5ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Interaction Loop\n",
        "1. Develop a main chat function that:\n",
        "   - Retrieves relevant long-term memories.\n",
        "   - Invokes the conversational chain with the current input and memories.\n",
        "   - Updates the long-term memory based on the interaction.\n",
        "   - Returns the AI's response."
      ],
      "metadata": {
        "id": "SmRuEPNnkWww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing and Refinement\n",
        " 1. Run example conversations to test the agent's ability to maintain context.\n",
        " 2. Review both short-term and long-term memories after interactions.\n",
        " 3. Adjust memory management criteria and prompt structure as needed."
      ],
      "metadata": {
        "id": "u3iZH4dlkmNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph  langchain_community langchain_core"
      ],
      "metadata": {
        "id": "oZOiWtJlU3io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import google.generativeai as gai\n",
        "from google.colab import userdata\n",
        "gai.configure(api_key = userdata.get('gemini_api'))\n",
        "llm =   gai.GenerativeModel(\"gemini-pro\")\n"
      ],
      "metadata": {
        "id": "MzIOxMpUWim8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Memory Stores**  \n",
        "\n",
        "We will create both short-term( chat history )  and long term memory.\n"
      ],
      "metadata": {
        "id": "Sa8xmViMnSOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_store = {}\n",
        "long_term_memory = {}\n",
        "def get_chat_history(session_id) :\n",
        "   if session_id not in chat_store :\n",
        "     chat_store[session_id] = ChatMessageHistory()\n",
        "   return chat_store[session_id]\n",
        "\n",
        "def update_long_term_memory(session_id : str , input: str , output : str) :\n",
        "  if session_id not in long_term_memory :\n",
        "      long_term_memory[session_id] = []\n",
        "  if len(input) :\n",
        "    long_term_memory[session_id].append(f\" User said: {input} \")\n",
        "  if len(long_term_memory[session_id]) > 5 :\n",
        "      long_term_memory[session_id][-5:]   # last  5 elements\n",
        "\n",
        "\n",
        "def get_long_term_memory(session_id : str ) :\n",
        "    return \" , \".join(long_term_memory.get(session_id , []) )"
      ],
      "metadata": {
        "id": "-5Fr8YN1ck4l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template"
      ],
      "metadata": {
        "id": "tzPGQWF-tGRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the information from long-term memory if relevant.\"),\n",
        "    (\"system\", \"Long-term memory: {long_term_memory}\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "a4Vxdc2hsLP5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational Chain"
      ],
      "metadata": {
        "id": "A1pVfRzOu2hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('open_ai_api')\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
      ],
      "metadata": {
        "id": "h_6_86NwvCXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  prompt  |  llm\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain ,\n",
        "    get_chat_history ,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "F-rl1Mt8zkNy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Function"
      ],
      "metadata": {
        "id": "onP-FHHm1Fhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat(input_text : str  , session_id : str ) :\n",
        "  long_term_mem =  get_long_term_memory(session_id)\n",
        "  response = chain_with_history.invoke({\n",
        "      \"input\" :  input_text , \"long_term_memory\" :long_term_mem\n",
        "  }, config={\"configurable\": {\"session_id\": session_id}})\n",
        "  update_long_term_memory(session_id, input_text, response.content)\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "gFVzM0p01ECS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user_123\"\n",
        "\n",
        "print(\"AI:\", chat(\"Hello! My name is Alice.\", session_id))\n",
        "print(\"AI:\", chat(\"What's the weather like today?\", session_id))\n",
        "print(\"AI:\", chat(\"I love sunny days.\", session_id))\n",
        "print(\"AI:\", chat(\"Do you remember my name?\", session_id))"
      ],
      "metadata": {
        "id": "u13YRNJb3hD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review Memory\n"
      ],
      "metadata": {
        "id": "-Gq88pG03Z33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Conversation History:\")\n",
        "for message in chat_store[session_id].messages:\n",
        "    print(f\"{message.type}: {message.content}\")\n",
        "\n",
        "print(\"\\nLong-term Memory:\")\n",
        "print(get_long_term_memory(session_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaJcJHsG3bKK",
        "outputId": "c882b048-a553-4cd7-8c7c-93d83115c7c4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation History:\n",
            "\n",
            "Long-term Memory:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEfReBen3dIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}